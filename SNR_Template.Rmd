---
title: "Simple Normal Regression Analysis"
author: "`r Sys.info()[['effective_user']]`"
date: "`r Sys.time()`"
output: 
  html_document:
    theme: paper
params:
  data: !r dplyr::tibble()
  x: ""
  y: ""
  n_chains: 4
  n_iter: 10000
  ci_pct: 95
  n_lines: 0
  posterior_predict: !r c()
  n_folds: 10
  func: ""
  
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.width = 9, fig.height = 4)
options(scipen = 100)
library(tidyverse)
library(rstanarm)
library(bayesplot)
library(reshape2)
library(janitor)
library(bayesrules)
library(kableExtra)

dataset <- na.omit(params$data)


if(params$n_lines == 0){
  num_lines_display <- round(nrow(dataset)/4)
}else{
  num_lines_display <- params$n_lines
}
```


## Analysis Inputs

#### Data: 

```{r, echo = FALSE}
str(dataset)
```

#### Independent Variable (X): <span style="color:gray">`r params$x`</span>

#### Depedent Variable (Y): <span style="color:gray">`r params$y`</span>

#### Number Of Stan Chains: <span style="color:gray">`r params$n_chains`</span>

#### Number Of Iterations Per Chain: <span style="color:gray">`r params$n_iter`</span>

#### Credible Interval Percentage: <span style="color:gray">`r params$ci_pct`</span>

#### Number Of Potential Regression Lines To Graph: <span style="color:gray">`r num_lines_display`</span>


```{r, eval = length(params$posterior_prob)> 0 , echo = FALSE}
knitr::asis_output("#### Conditions To Check Posterior Probability Of (For Y Variable):")

params$posterior_prob

```



```{r, eval = length(params$posterior_predict)> 0, echo = FALSE}
knitr::asis_output("#### Values For X With Which To Predict Posterior Distributions Of Y: ")

params$posterior_predict

```

#### Number Of Folds To Use With Cross-Validation: <span style="color:gray">`r params$n_folds`</span>


```{r, echo = FALSE}
# Define CV function

prediction_summary_cv <- function(data, model, k = 10, prob_inner = 0.5, prob_outer = 0.95){
  
      # Split data into k possibly unequal folds
      # https://gist.github.com/dsparks/3695362
      random_draw <- rnorm(nrow(data))
      k_quantiles <- quantile(random_draw, 0:k/k)
      folds <- cut(random_draw, k_quantiles, include.lowest = TRUE)
      levels(folds) <- 1:k
      data <- data %>% 
                mutate(fold = sample(folds, size = length(folds), replace = FALSE))
      y <- model$terms[[2]]
          
      # Test the model on each one of the k folds
      folds <- data.frame()
      for(i in 1:k){
        
         data_train <- data %>% 
           filter(fold != i) %>% 
           dplyr::select(-fold)
         
        data_test <- data %>% 
           filter(fold == i) %>% 
           dplyr::select(-fold)
        
        model_train <- update(model, data = data_train, refresh = FALSE)
        
        predictions_test <- posterior_predict(model_train, newdata = data_test)
        
        folds <- rbind(folds,
                       prediction_summary(y = c(as.matrix((data_test %>% select(y))[,1])),
                                          yrep = predictions_test))
        }
          
      # Calculate the cross validated error
      cv <- folds %>% 
        summarize_all(mean)
      
      folds <- data.frame(fold = 1:k, folds)
      
      
      return(list(folds = folds, cv = cv))
}

```


## Plot Of X & Y



Below, we see a scatterplot of our X & Y variables of interest, `r params$x` and `r params$y`:



```{r}

ggplot(dataset, aes_string(x = params$x, y = params$y)) + 
  geom_point(color = "#00134A") +
  ggtitle(paste0(params$y, " vs. ", params$x))

```

## Defining The Stan Model



We then use the `Stan` package to build a Bayesian Simple Normal Regression Model based on the provided inputs:



```{r message=FALSE, results="hide", class.source = "fold-show", echo = TRUE}
set.seed(84735)

## Convert character parameters to a formula

form <- as.formula(paste0(params$y, " ~ ", params$x))

## Pass that formula to normal_model_sim

normal_model_sim <- stan_glm(form, 
  data = dataset,
  family = gaussian,
  chains = params$n_chains,
  iter = params$n_iter)

```


## Visual Diagnostics



To evaluate the quality of the model, we consider several visual diagnostics: trace plots and chain density overlay plots.



#### Trace Plots



Trace plots show us the patterns of our chain values across all of their iterations. Based on the appearance of those plots, we can determine whether the model was an effective simulation or not.

What you **want** to see:

* A bunch of white noise

What you **DO NOT want** to see:

* A clear non-zero slope in the movement of the chains across all iterations. For example, as iterations increase, you can see that the chain values consistently increase/decrease. 

* Sections where the chain appears to get stuck. These will be visible as breaks in the plot where there is only a horizontal line rather than vertical white noise.

**If the trace plots are of low quality, INCREASE the number of iterations for your Stan model.**

```{r}
# Trace plots of parallel chains
mcmc_trace(normal_model_sim, size = 0.1)
```



#### Chain Density Overlay Plots



Chain density overlay plots also help determine whether the simulation was accurate. These show overlaid density plots of the posterior distributions found all of our parameters in each chain.

What you **want** to see:

* For each parameter (intercept, slope, sigma), the density plots should look almost identical. 
* It is okay if there are slight deviations in density as long as the peaks of the density plots are very similar, as are their ranges and shapes.

What you **DO NOT want** to see:

* Chains with different distributions (different peaks, ranges, and/or shapes)

**If the density overlay plots are of low quality, INCREASE the number of iterations for your Stan model.**

```{r}
# Density plots of parallel chains
mcmc_dens_overlay(normal_model_sim)
```




## Posterior Summary Statistics and Mathematical Diagnostics



Below are the summary statistics for the posterior of our simple normal regression model:




```{r, echo = FALSE}
# Posterior summary statistics
model_summary <- summary(normal_model_sim)
output <- head(as.data.frame(model_summary), -2)

output %>%
  kable(booktabs = T) %>%
  kable_styling() 
```

The main columns to pay attention to are:

1. mean
2. n_eff
3. Rhat

Using these three columns, we can determine the *average* regression equation across all of our iterations (using 'mean') as well as get a sense of whether or not the chains produced an accurate estimate of the posterior (using 'n_eff' and 'Rhat').



```{r, echo = FALSE}
df <- head(as.data.frame(model_summary), -2)
intercept <- round(df[[1]][1], 2)
slope <- round(df[[1]][2], 2)
Rhats <- df[[8]]
n_effs <- df[[7]]

if(slope > 0){
  slope <- paste0("+ ", as.character(slope))
}else if(slope < 0) {
  slope <- paste0("- ", as.character(abs(slope)))
}else{
  slope <- paste("+ ", as.character(slope))
}

```


#### Best Guess For Regression Equation ('mean'):



$$\hat{\text{`r params$y`}} = `r intercept` `r slope` * \text{`r params$x`}$$


#### Number of Effective Iterations ('n_eff'):



N_eff quantifies the number of independent samples it would take to produce an equivalently accurate posterior approximation as created by the Stan model.

We should be **suspicious** of the quality of our model if N_eff is less than 10% of the number of iterations run by our model across all of its chains.

**In the table below, parameters for which suspicious values of n_eff were found are highlighted red. If NO rows are red, then your model is likely okay in terms of its number of effective iterations!**

```{r, echo = FALSE}
n_effs_pretty <- df[7]

n <- (params$n_iter * params$n_chains)

n_effs_flag <- n_effs_pretty %>%
  mutate(n_iterations = n) %>%
  mutate(ratio = n_eff / n_iterations) %>%
  mutate(`Suspicious?` = n_eff/n < 0.1)

row.names(n_effs_flag) <- c("(Intercept)", params$x, "sigma")

color.me <- which(n_effs_flag$`Suspicious?` == TRUE)

n_effs_flag %>%
  kable(booktabs = T) %>%
  kable_styling() %>%
  row_spec(color.me, bold = T, color = "red")
```



#### Variability Across vs Within Chains ('Rhat'):



Rhat measures the ratio between:

1. The combined variability in our parameter across all chains, and
2. The typical variability within any individual chain.

Rhat values $\approx$ 1 indicate **stability** across chains, while Rhat values > 1 indicate **instability**.


**In the table below, parameters for which suspicious values of Rhat were found are highlighted red. If NO rows are red, then your model is likely okay in terms its chain variability ratio!**


```{r, echo = FALSE}
Rhats_pretty <- df[8]
Rhats <- df[[8]]

Rhats_flagged <- Rhats_pretty %>% 
  mutate(`Unstable?` = Rhat > 1.05)

row.names(Rhats_flagged) <- c("(Intercept)", params$x, "sigma")

color.me <- which(Rhats_flagged$`Unstable?` == TRUE)

Rhats_flagged %>% 
  kable(booktabs = T) %>%
  kable_styling() %>%
  row_spec(color.me, bold = T, color = "red")

```



## Credible Intervals



Below we see `r params$ci_pct`% Credible Intervals for the posterior distribution of our three parameters: the intercept, slope, and the standard deviation of our data around the proposed regression line.

```{r}
if (params$ci_pct > 1) {
  frac <- params$ci_pct / 100
} else {
  frac <- params$ci_pct
}

credible_intervals <- posterior_interval(normal_model_sim, prob = frac) 

credible_intervals %>%
  kable(booktabs = T) %>%
  kable_styling() 
```



```{r, fig.show = 'hold', out.width = "30%"}
# Shade in the CI. For example:

mcmc_areas(normal_model_sim,
  pars = "(Intercept)",
  prob = frac,
  point_est = "mean"
) + labs(title = "(Intercept)") + 
  theme(text = element_text(size = 30)) 

mcmc_areas(normal_model_sim,
  pars = c(params$x),
  prob = frac,
  point_est = "mean"
) + labs(title = params$x) +
  theme(text = element_text(size = 30)) 

mcmc_areas(normal_model_sim,
  pars = "sigma",
  prob = frac,
  point_est = "mean"
) + labs(title = "sigma") + 
  theme(text = element_text(size = 30)) 



```

There is a `r params$ci_pct`% posterior probability that the:

- **intercept** of the model is between `r round(credible_intervals[[1]], 2)` and `r round(credible_intervals[[4]], 2)`.
- **slope** (`r params$x`) of the model is between `r round(credible_intervals[[2]], 2)` and `r round(credible_intervals[[5]], 2)`.
- **sigma** (standard deviation) of the model is between `r round(credible_intervals[[3]], 2)` and `r round(credible_intervals[[6]], 2)`.


## Plot of Potential Regression Lines Determined By Model

```{r}
# Restructuring the data frame
normal_model_df <- as.array(normal_model_sim) %>%
  melt() %>%
  pivot_wider(names_from = parameters, values_from = value)


if(params$n_lines == 0){
  num_lines <- round(nrow(dataset)/4)
}else{
  num_lines <- params$n_lines
}

first_n <- head(normal_model_df, num_lines)
intercept <- model_summary[1][1]
slope <- model_summary[2][1]
```

Below is a plot of the first `r num_lines` potential regression lines proposed by the model (plotted in light blue) alongside the mean proposed regression line (plotted in dark blue):

```{r}


ggplot(dataset, aes_string(x = params$x, y = params$y)) +
  geom_point(size = 0.1) +
  geom_abline(
    data = first_n,
    aes_string(
      intercept = "`(Intercept)`",
      # issue here: have to access the x variable from the first_20 df not main df?
      slope = params$x
      ),
    color = "#CADDEC", alpha = .8, size = 0.1
  )+
  geom_abline(aes(intercept = intercept, slope = slope),
    color = "#00134A"
  )
```

## Posterior Probability of Calculated Slope

```{r}
slope_direction <- ifelse(df[[1]][2] > 0, "positive", "negative")
slope_condition <- ifelse(slope_direction == "positive", " > 0", " < 0")
```

```{r}

to_eval <- parse(text = paste0(params$x, slope_condition))
  
posterior_probability <- normal_model_df %>%
  mutate(fits_condition = eval(to_eval)) %>%
  tabyl(fits_condition)

```

The slope found by the model was `r slope_direction`. The probability of getting a slope `r slope_condition` is `r ifelse(is.na(posterior_probability[[3]][2]) , posterior_probability[[3]], posterior_probability[[3]][2])`.

```{r}
posterior_probability %>%
  kable(booktabs = T) %>%
  kable_styling() 
```




```{r, echo = FALSE, eval = length(params$posterior_predict)>0}
knitr::asis_output("## Posterior Predictions \n")

knitr::asis_output(paste0("Below are the predicted posterior distributions of ", params$y, " when ", params$x, " = the following value(s): "))

knitr::asis_output(paste(params$posterior_predict, collapse = ", "))

```

$~$

```{r, echo = FALSE, eval = length(params$posterior_predict)>0, fig.show = 'hold', out.width = paste0(floor(100/length(params$posterior_predict)), "%")}


set.seed(84735)

for(prediction in params$posterior_predict){
  to_eval <- parse(text = paste0(params$x, " = ", prediction))
  shortcut_prediction <- posterior_predict(
  normal_model_sim,
  newdata = data.frame(eval(to_eval)))

  graph <- mcmc_dens(shortcut_prediction) +
    labs(x = paste0("Predicted ", params$y, " when ", params$x, " = ", prediction)) +     
    theme(text = element_text(size = 30)) 
  
  print(graph)
}

```


## Evaluating Prediction Quality



#### Posterior Predictive Check

The posterior predictive check compares the distribution of our data (dark blue) with the distribution of our predictions (light blue).

We have a high quality model if the following features our distributions are similar:

* Range
* Mode 
* Shape


```{r}
set.seed(84735)
pp_check(normal_model_sim, nreps = 50)
```



#### Posterior Prediction Intervals

The graph below provides a quick visual summary of how consistent the observed outcomes/data (dark blue dots) are with the range of predictions made by posterior model (light blue lines). 

The model is of high quality if:

* The vast majority of dark blue dots fall within the light blue bars, AND
* Within the light blue region, the dots cluster more toward the middle than toward the outside.

```{r}
set.seed(84735)

predictions <- posterior_predict(normal_model_sim,
  newdata = dataset)


to_eval <- paste0("ppc_intervals(dataset$", params$y, ", yrep = predictions, x = ", "dataset$", params$x, ", prob = 0.5, prob_outer = 0.95)")

eval(parse(text = to_eval))
```



#### Cross Validation

Finally, we can use cross-validation to get a sense of how well our model will work when given entirely new data!

Below, we see summaries of model quality for a cross-validation model with `r n_folds` folds:

```{r, message = FALSE}
set.seed(84735)

cv_procedure <- prediction_summary_cv(data = dataset,
                                      model = normal_model_sim,
                                      k = params$n_folds)

cv_procedure$cv %>%
  kable(booktabs = T) %>%
  kable_styling() 
```

* The `mae` (median absolute error) value means that the typical difference between the observed data points and their posterior predictive medians is `r cv_procedure$cv[[1]]`.

* The `mae_scaled` (scaled median absolute error) value means that the typical number of absolute deviations between the observed data points and their posterior predictive medians is `r cv_procedure$cv[[2]]`

* The `within_50` and `within_95` statistics measure the proportion of observed values that fall within the 50% and 95% posterior prediction intervals, respectivelyl. For this model, those values are `r cv_procedure$cv[[3]]` (50%) and `r cv_procedure$cv[[4]]` (95%).


High quality models have the following characteristics:

* `mae` values are dependent on the scale of the data. They should be as small as possible.
* `mae_scaled` should be relatively close to 1 (and definitely less than 2).
* `within_50` should be around 0.5 (higher values are better).
* `within_95` should be around 0.95 (higher values are better).


# To-Do:


1. README + sample output HTMLs
2. See if we can accept things like x = temp_feel instead of x = "temp_feel" (highly optional)
3. Think about removing/changing posterior probability section because it doesn't make much sense right now.
4. For posterior predictions, consider adding in some visuals to help differentiate between graphs.